## Moonshine LoRA

Fine-tunes a Moonshine checkpoint using LoRA. 

### Setup

Prefer `make` targets when available.

FFmpeg is required for audio decoding (used by `datasets`/`torchcodec`). Install it with Homebrew:

```
brew install ffmpeg
```

- `make venv`
- `source .venv/bin/activate`
- `make sync`

### Run

Prefer `make` targets when available.

- `HF_TOKEN=... make run` (required if the Moonshine repo is gated)

Small real-world run:

- `HF_TOKEN=... uv run python -m lora.runners.real_small`

Artifact STT check:

- `uv run python scripts/build_manifest.py --split test --samples 5 --output data/heldout_manifest.jsonl`
- `just transcribe "--model-id UsefulSensors/moonshine-tiny --adapter-dir outputs/real_small/lora_adapter --processor-dir outputs/real_small/processor --manifest data/heldout_manifest.jsonl --output outputs/real_small/artifact_test.json"`

Artifacts are written to `outputs` by default.

## Voice CLI (Interactive)

We include a standalone CLI tool for live voice interaction with the models.

- **Location:** `packages/lora-cli`
- **Features:** Push-to-Talk, terminal UI, live transcription.
- **Usage:**
  ```bash
  cd packages/lora-cli
  uv sync
  uv run moonshine
  ```

See [packages/lora-cli/README.md](packages/lora-cli/README.md) for full details.

## Development Philosophy

This project strictly adheres to a **fail-fast, no-fallback** engineering philosophy:
- Do not write defensive `try...except` blocks that swallow errors.
- Do not implement silent fallbacks for missing configurations, unexpected data shapes, or missing dependencies.
- If a contract is violated, raise an explicit exception (`ValueError`, `KeyError`, `RuntimeError`, etc.) immediately. We prefer the application to crash loudly rather than proceed in an ambiguous state.

## Documentation

### Training Guide

Use `docs/training.md` for full requirements, evaluation plan, and the report
template to capture results for each run.

### Glossary

- Domain manifest: Primary evaluation dataset for domain-shifted audio (`data/domain_manifest.jsonl`).
  - Generated by `scripts/build_domain_manifest.py`, for example:
    - `uv run python scripts/build_domain_manifest.py --config other --split test --samples 200 --output data/domain_manifest.jsonl --seed 42`
- Heldout manifest: Safety/guardrail evaluation dataset for non-target regression checks (`data/heldout_manifest.jsonl`).
  - Generated by `scripts/build_manifest.py`, for example:
    - `uv run python scripts/build_manifest.py --split test.clean --samples 120 --output data/heldout_manifest.jsonl`
- Baseline: Metrics from the base model before LoRA training on a chosen manifest.
  - Generated during experiment runs in `src/lora/runners/experiment.py` (`baseline_wer`, `baseline_eval_loss`) and written to `experiment_metrics.json` under the run output directory.
- Tuned model: Base model with trained LoRA adapter applied.
  - Generated by training runs (`src/lora/runners/experiment.py`) as artifacts in:
    - `outputs/<run_id>/lora_adapter`
    - `outputs/<run_id>/processor`
- Guardrail metric: Metric used to block regressions while optimizing the primary target (in this repo, heldout WER).
  - Generated from evaluation output on `data/heldout_manifest.jsonl` (same experiment metrics pipeline).
- Headroom: Remaining achievable improvement on a dataset (often estimated by baseline vs tuned deltas across runs).
  - Derived from experiment results tables and metrics; this is an analysis concept, not a standalone generated file.
- Normalization/decode parity: Using matching normalization and decode settings across training/eval/inference.
  - Enforced by configuration and code paths (for example, shared decode setup in `src/lora/model_utils.py` and inference path in `src/lora_training/transcribe.py`); conceptual, not a standalone artifact.
- Manifest: JSONL file describing samples (audio arrays + transcript metadata) used for training/evaluation/inference.
  - Generated by manifest scripts such as:
    - `scripts/build_manifest.py`
    - `scripts/build_domain_manifest.py`
    - `scripts/merge_manifests.py`

### Recommended Run Flow

1. Prepare dataset and confirm splits.
2. Evaluate baseline metrics on validation data.
3. Train LoRA adapter and capture metrics.
4. Re-evaluate and compare to success criteria.
5. Record results using the training report template.

### Artifact Contract

Unified experiment runs (`src/lora/runners/experiment.py`) write these outputs under `--output-dir`:
- `experiment_metrics.json`: baseline/tuned WER and loss, runtime, and run settings.
- `lora_adapter/`: LoRA adapter checkpoint.
- `processor/`: processor snapshot used for parity with evaluation/inference.

Small real-world runs (`src/lora/runners/real_small.py`) write:
- `real_metrics.json`
- `lora_adapter/`
- `processor/`

### Manifest Schema

Manifest files are JSONL. Each line is one sample object with:
- `audio`: `list[float]` waveform samples.
- `text`: transcript string.
- `speaker_id`: optional speaker identifier (int/string), used when available for speaker-aware splitting.
